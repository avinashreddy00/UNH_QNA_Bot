{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7068792,"sourceType":"datasetVersion","datasetId":4070572}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport tqdm\nimport torch\n\nclass ChatData(Dataset):\n    def __init__(self, path: str, tokenizer):\n        self.data = pd.read_csv(path)\n\n        self.X = []\n        for index, row in self.data.iterrows():\n            self.X.append(\"<startofstring> \" + row['question'] + \" <bot>: \" + row['answer'] + \" <endofstring>\")\n\n        self.X = self.X[:5000]\n\n        self.X_encoded = tokenizer(self.X, max_length=40, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n        self.input_ids = self.X_encoded['input_ids']\n        self.attention_mask = self.X_encoded['attention_mask']\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return (self.input_ids[idx], self.attention_mask[idx])\n\ndef train(chatData, model, optim):\n    epochs = 200\n\n    for i in tqdm.tqdm(range(epochs)):\n        for X, a in chatData:\n            X = X.to(device)\n            a = a.to(device)\n            optim.zero_grad()\n            loss = model(X, attention_mask=a, labels=X).loss\n            loss.backward()\n            optim.step()\n        torch.save(model.state_dict(), \"model_state.pt\")\n        print(infer(\"How can my commuter student get involved?\"))\n\ndef infer(inp):\n    inp = \"<startofstring> \" + inp + \" <bot>: \"\n    inp = tokenizer(inp, return_tensors=\"pt\")\n    X = inp[\"input_ids\"].to(device)\n    a = inp[\"attention_mask\"].to(device)\n    output = model.generate(X, attention_mask=a)\n    output = tokenizer.decode(output[0])\n    return output\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\ntokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n                                \"bos_token\": \"<startofstring>\",\n                                \"eos_token\": \"<endofstring>\"})\ntokenizer.add_tokens([\"<bot>:\"])\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\")\nmodel.resize_token_embeddings(len(tokenizer))\nmodel = model.to(device)\n\nchatData = ChatData(\"/kaggle/input/unh-qa/qa_unh.csv\", tokenizer)\nchatData = DataLoader(chatData, batch_size=64)\n\nmodel.train()\n\noptim = Adam(model.parameters(), lr=1e-3)\n\nprint(\"training .... \")\ntrain(chatData, model, optim)\n\nprint(\"infer from model : \")\nwhile True:\n    inp = input()\n    print(infer(inp))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T15:45:03.362254Z","iopub.execute_input":"2023-12-05T15:45:03.362552Z"},"trusted":true},"execution_count":null,"outputs":[]}]}